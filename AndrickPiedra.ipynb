{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tMbyP-Unv-b"
      },
      "outputs": [],
      "source": [
        "import glob \n",
        "import pandas as pd \n",
        "import xml.etree.ElementTree as ET \n",
        "from datetime import datetime\n",
        "import requests\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_data():\n",
        "    # Define the remote file to retrieve\n",
        "    remote_url = 'http://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0221EN-SkillsNetwork/labs/module%206/Lab%20-%20Extract%20Transform%20Load/data/datasource.zip'\n",
        "    # Define the local filename to save data\n",
        "    local_file = 'datasource.zip'\n",
        "    # Make http request for remote file data\n",
        "    data = requests.get(remote_url)\n",
        "    # Save file data to local copy\n",
        "    with open(local_file, 'wb') as f:\n",
        "        f.write(data.content)\n",
        "    with ZipFile(local_file, 'r') as zipObj:\n",
        "        # Extract all the contents of zip file in different directory\n",
        "        zipObj.extractall('dealership_data')\n",
        "        \n",
        "tmpfile = \"dealership.temp.tmp\"\n",
        "logfile = \"dealership_logfile.txt\" \n",
        "targetfile = \"dealership_transformed_data.csv\"\n",
        "\n",
        "def extract_from_csv(file_to_process): \n",
        "    dataframe = pd.read_csv(file_to_process) \n",
        "    return dataframe\n",
        "\n",
        "def extract_from_json(file_to_process):\n",
        "    dataframe = pd.read_json(file_to_process, lines=True)\n",
        "    return dataframe\n",
        "\n",
        "def extract_from_xml(file_to_process):\n",
        "    dataframe = pd.DataFrame(columns=['car_model','year_of_manufacture','price', 'fuel'])\n",
        "    tree = ET.parse(file_to_process) \n",
        "    root = tree.getroot() \n",
        "    for person in root: \n",
        "        car_model = person.find(\"car_model\").text \n",
        "        year_of_manufacture = int(person.find(\"year_of_manufacture\").text)\n",
        "        price = float(person.find(\"price\").text) \n",
        "        fuel = person.find(\"fuel\").text \n",
        "        dataframe = dataframe.append({\"car_model\":car_model, \"year_of_manufacture\":year_of_manufacture, \"price\":price, \"fuel\":fuel}, ignore_index=True) \n",
        "    return dataframe\n",
        "    \n",
        "def extract():\n",
        "    extracted_data = pd.DataFrame(columns=['car_model','year_of_manufacture','price', 'fuel']) \n",
        "    #for csv files\n",
        "    for csvfile in glob.glob(\"dealership_data/*.csv\"):\n",
        "        extracted_data = extracted_data.append(extract_from_csv(csvfile), ignore_index=True)\n",
        "    #for json files\n",
        "    for jsonfile in glob.glob(\"dealership_data/*.json\"):\n",
        "        extracted_data = extracted_data.append(extract_from_json(jsonfile), ignore_index=True)\n",
        "    #for xml files\n",
        "    for xmlfile in glob.glob(\"dealership_data/*.xml\"):\n",
        "        extracted_data = extracted_data.append(extract_from_xml(xmlfile), ignore_index=True)\n",
        "    return extracted_data\n",
        "   \n",
        "def transform(data):\n",
        "    data['price'] = round(data.price, 2)\n",
        "    return data\n",
        "\n",
        "def load(targetfile, data_to_load):\n",
        "    data_to_load.to_csv(targetfile)\n",
        "\n",
        "def log(logfile, message):\n",
        "    timestamp_format = '%H:%M:%S-%h-%d-%Y'\n",
        "    #Hour-Minute-Second-MonthName-Day-Year\n",
        "    now = datetime.now() # get current timestamp\n",
        "    timestamp = now.strftime(timestamp_format)\n",
        "    with open(logfile,\"a\") as f: \n",
        "        f.write('[' + timestamp + ']: ' + message + '\\n')\n",
        "        print(message)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    logfile    = \"dealership_logfile.txt\"            # all event logs will be stored\n",
        "    targetfile = \"dealership_transformed_data.csv\"   # transformed data is stored\n",
        "\n",
        "    log(logfile, \"Download Used Car Data\")\n",
        "    get_data()\n",
        "    \n",
        "    log(logfile, \"ETL Job Started\")\n",
        "\n",
        "    log(logfile, \"Extract phase Started\")\n",
        "    extracted_data = extract() \n",
        "    log(logfile, \"Extract phase Ended\")\n",
        "\n",
        "    log(logfile, \"Transform phase Started\")\n",
        "    transformed_data = transform(extracted_data)\n",
        "    log(logfile, \"Transform phase Ended\")\n",
        "\n",
        "    log(logfile, \"Load phase Started\")\n",
        "    load(targetfile, transformed_data)\n",
        "    log(logfile, \"Load phase Ended\")\n",
        "\n",
        "    log(logfile, \"ETL Job Started\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty1EG7kun2qk",
        "outputId": "dd84de30-4fc1-4f4a-9dfd-f8c2e530ec85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download Used Car Data\n",
            "ETL Job Started\n",
            "Extract phase Started\n",
            "Extract phase Ended\n",
            "Transform phase Started\n",
            "Transform phase Ended\n",
            "Load phase Started\n",
            "Load phase Ended\n",
            "ETL Job Started\n"
          ]
        }
      ]
    }
  ]
}